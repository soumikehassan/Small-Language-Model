## ðŸ§  Backbone of Transformer Architecture

The **Transformer** is a groundbreaking neural network architecture based on self-attention mechanisms. Its **backbone** refers to the core layers and operations that make Transformers powerful and flexible for NLP and beyond.

---

### ðŸ”© Core Components

#### 1. **Input Embeddings**
- Converts tokens (words/subwords) into dense vectors.
- Includes **Positional Encoding** to retain the order of tokens, since Transformers are non-sequential by design.

#### 2. **Transformer Block (Backbone Unit)**
Each Transformer block is composed of:

- âœ… **Multi-Head Self-Attention**
  - Allows the model to attend to different parts of the input simultaneously.
  - Learns contextual relationships.

- âœ… **Layer Normalization**
  - Stabilizes training and accelerates convergence.

- âœ… **Feedforward Neural Network (FFN)**
  - A fully connected MLP applied to each position.
  - Commonly uses ReLU or GELU activation.

- âœ… **Residual Connections**
  - Skip connections to preserve gradients and improve learning in deeper models.

#### 3. **Stacked Layers**
- Transformers consist of **stacked encoder and/or decoder blocks**.
- Number of layers (depth) and attention heads are configurable.

---

### ðŸ”„ Variants Based on Backbone

- **BERT** â€“ Uses only the **encoder** part.
- **GPT** â€“ Uses only the **decoder** part.
- **T5 / BART** â€“ Use both encoder and decoder.

---

### ðŸ§  Summary

> The backbone of a Transformer is a **stack of attention + feedforward blocks**, enriched with **positional information** and **residual connections**. This architecture is what powers large language models like GPT, BERT, T5, etc.

---

ðŸ“Œ _Want to dive deeper? Read the original paper: [Attention is All You Need](https://arxiv.org/abs/1706.03762)_

